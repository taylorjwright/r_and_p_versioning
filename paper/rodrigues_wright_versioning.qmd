---
bibliography: rodrigues_wright_versioning.bib
format: 
  pdf:
    number-sections: true
geometry: margin = 1.0in
font-size: 11pt
mainfont: cochineal
sansfont: "TeX Gyre Heros"
header-includes:
  \usepackage{booktabs}
  \usepackage{setspace}
  \doublespacing
  \usepackage[left]{lineno}
  \linenumbers
---

\date{\today}
\title{\textbf{Replication materials are great but software versioning still poses a problem for open science}}
\author{Taylor J. Wright\thanks{Brock University, \href{mailto:twright3@brocku.ca}{twright3@brocku.ca}} \and Bruno Rodrigues \thanks{Ministry of Higher Education and Research, Luxembourg}
}

```{r, echo=FALSE, include=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(ggplot2)
library(flextable)
```

\maketitle

\newpage

## Introduction

Beginning in the early 2010s with psychology and quickly spilling over into political science and economics on the backs of high profile cases of fraud (@broockman2015irregularities) and errors (@herndon2014does), the "replication crisis" in the social sciences has become mainstream, even finding its way into the popular press (@aschwanden2015, @gelman2018). 
Concerns over how well published results hold up to replication in other settings has lead to large scale initiatives to document how trustworthy the existing stock of evidence is (e.g. @camerer2016evaluating, @chang2022replicable, and @open2015estimating). 
While the *replicability* of studies has recieved much attention, there has been a tandem movement discussing the importance of and need for research *reproducibility*. 
While the exact scope and definition is subject to ongoing debate, replication is, broadly speaking, re-testing a hypothesis while changing an element of previous research (e.g. the sample or estimating equation) whereas reproducibility (sometimes referred to as verification or computational reproducibility) is following a study's protocol or methods exactly and obtaining the results presented in the study.^[For further discussion on definitions, scopes, and types of replication and reproducibility see, for example, @clemens2017meaning, @derksen2022kinds, @hamermesh2007replication, and @nosek2020replication.] 
In our view, reproducibility is an insufficient but necessary condition for replication---that is, work cannot be replicable if it is not reproducible and it likely does not make sense to spend resources on replication if research is not in the first place reproducible.   

These concerns about the reproducibility (and replicability) of social science research have prompted a push, often referred to as Data Access and Research Transparency (DA-RT) in political science following @lupia2014openness, for journals to require the publication of research materials that accompany academic research.^[See, for example, the 2014 Joint Editors Transparency Statement which was signed by editors of 27 leading political science journals: \url{https://www.dartstatement.org/2014-journal-editors-statement-jets}] 
Specifically, the provision and publication of underlying data and code used for data preparation and analysis. 
This push has seen some success, with a growing number journals now requiring the provision of replication materials as a condition of publication.
Some journals, such as the American Econonmic Review (AER) or American Journal of Political Science (AJPS), even have verification policies that require authors upload their replication materials and have their results verified by another team (either the journal's replication team (AER) or a third party, (AJPS)) prior to publication.
However, even if these materials are provided, and even when in place these policies do not have perfect compliance (@philip2010report, @stockemer2018data), regular software updates and new version releases can result in the replication materials failing to faithfully reproduce the authors' results or even run at all. ^[@simonsohn95GroundhogAddressing2021 presents several examples of \texttt{R} changes that could break scripts.]

In this paper we present a case study of an article published in Journal of Politics in January 2022 titled, ``Multiracial Identity and Political Preferences'', @davenport2022multiracial, that details that replication challenges arising from changes in the statistical software \texttt{R}. 
We were unable to reproduce the authors' results using either the current version of \texttt{R}, or the version that the authors indicate they used.
The lack of reproducibility arose due to a change in the defaults used by base \texttt{R} when generating random numbers starting in version 3.6.0.

We contribute to the existing literature... 
Strand 1: Discussion of importance of availablility of replication/reproducibility materials
Contribution 1: A necessary but insufficient condition 
Strand 2: Discussion of problems raised by software and package versioning for reproducibility
Contribution 2: A walk through of the problem with a concrete example
Strand 3: Discussion of tools and best practices for ensuring reproducibility
Contribution 3: Step-by-step guide to using Docker, {renv}, and {targets} to ensure reproducibility

The rest of the article proceeds as follows: Section 2 walks through the reproducibility issues in @davenport2022multiracial; Section 3 discusses currently available tools and best practices (e.g. Docker and \texttt{R} packages such as \texttt{renv}, \texttt{groundhog}) for ensuring that replication materials continue to faithfully reproduce research results, despite post-publication changes in the tools used; and Section 4 concludes.

\newpage

## Reproduction

### Illustrating the issue with software versioning 
A key thing here is that I don't think we want to be too hostile sounding towards these authors, they had readme files and reproducibility materials available. It's just that manual entry human-error hobgoblins got them with the R versioning.

- Brief discussion of authors' paper and context
- The results of their code using stated software version in documentation (just screenshot right now)
```{r fig1screenshot, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure1_screenshot.png", error = FALSE)
```

```{r fig2screenshot, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure2_screenshot.png", error = FALSE)
```
- The results of their code using later software version (post 3.6.0)

```{r fig1post, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure1_postchange.png", error = FALSE)
```

```{r fig2post, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure2_postchange.png", error = FALSE)
```
- Results of their code using earlier software version (pre 3.6.0)
```{r fig1pre, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure1_prechange.png", error = FALSE)
```

```{r fig2pre, echo=FALSE, out.width = '100%'}
# need to tweak axes to match authors, CI cap is truncated
knitr::include_graphics("./figures/figure2_prechange.png", error = FALSE)
```

The authors note that they use R version 3.6.2. Issue seems to be the weights the authors use are non-integer and Zelig uses sample() in that case which following changes to base R yields different results in 3.6.x and 3.5.x (see http://docs.zeligproject.org/articles/weights.html). Prior to R 3.6.x RNGkind(sample.kind = "Rounding") was the default behaviour but after 3.6.0 the sample function's new default behaviour is RNGkind(sample.kind = "Rejection") (see https://blog.revolutionanalytics.com/2019/05/whats-new-in-r-360.html). 

Soemthing about how there are ways to ensure that the estimates are consistent (changing the weights to integers or setting the RNGkind to be backwards compatible) but documenting the correct version of R used in the analysis is probably the easiest way. Segue into...

\newpage

## Discussion

### The problem is the seed?

The table below shows the quantiles of the means obtained from 100 runs with 
100 different random seeds on R version 3.5 for the `m2m` models. We should
check where the coefficients from the original paper fall in that distribution,
and see if coefficients should vary so much simply from changing the seed?

```{r echo = FALSE, message = FALSE}
simulations_m2m_paths <- list.files("../docker/version_3.x_many_seeds/original/shared_folder/", pattern = "m2m.*3_5.*\\d{1,}.rds", full.names = TRUE)

simulations_m2m <- tibble(paths = simulations_m2m_paths) %>%
  mutate(datasets = map(paths, readRDS),
         paths = str_extract(paths, "\\d{1,}.rds")) %>%
  unnest(cols = c(datasets))

simulations_m2m %>%
  group_by(race, model) %>%
  summarise(
    q_05_mean = round(quantile(mean, probs = 0.05), 2),
    q_20_mean = round(quantile(mean, probs = 0.2), 2),
    q_40_mean = round(quantile(mean, probs = 0.4), 2),
    q_50_mean = round(quantile(mean, probs = 0.5), 2),
    q_60_mean = round(quantile(mean, probs = 0.6), 2),
    q_80_mean = round(quantile(mean, probs = 0.8), 2),
    q_95_mean = round(quantile(mean, probs = 0.95), 2)
  ) %>%
  flextable()
```

The table below shows the quantiles of the means obtained from 100 runs with 
100 different random seeds on R version 3.5 for the `m2w` models:

```{r echo = FALSE, message = FALSE}
simulations_m2w_paths <- list.files("../docker/version_3.x_many_seeds/original/shared_folder/", pattern = "m2w.*3_5.*\\d{1,}.rds", full.names = TRUE)

simulations_m2w <- tibble(paths = simulations_m2w_paths) %>%
  mutate(datasets = map(paths, readRDS),
         paths = str_extract(paths, "\\d{1,}.rds")) %>%
  unnest(cols = c(datasets))

simulations_m2w %>%
  group_by(race, model) %>%
  summarise(
    q_05_mean = round(quantile(mean, probs = 0.05), 2),
    q_20_mean = round(quantile(mean, probs = 0.2), 2),
    q_40_mean = round(quantile(mean, probs = 0.4), 2),
    q_50_mean = round(quantile(mean, probs = 0.5), 2),
    q_60_mean = round(quantile(mean, probs = 0.6), 2),
    q_80_mean = round(quantile(mean, probs = 0.8), 2),
    q_95_mean = round(quantile(mean, probs = 0.95), 2)
  ) %>%
  flextable()

```

### Rebuilding the original development environment using Docker

Replicating results from past studies is quite challenging, for many reasons.
This article focuses on one of these reasons: results cannot be reproduced
because of changes introduced in more recent versions of the software used for
analysis, despite the availability of both data and replication scripts.

To replicate the results from the original study and to pinpoint the impact of
the change introduced in R 3.6.0, we chose to use Docker. Docker is a
containerization tool which enables one to build so-called *images*. These
images contain a software product alongside its dependencies and even pieces of
a Linux operating system. To use the software product, customers in turn only
need to be able to run Docker *containers* instantiated from the image
definition. Containerization tools such as Docker solved the "works on my
machine" problem: this problem arises when software that works well on the
development machine of the developer fails to run successfully on a customer's
machine. This usually happens because the customer's computer does not have the
necessary dependencies to run the software (which are traditionally not shipped
alongside the software product) or because of version mismatch of the operating
system.

A research project can also be seen as a *software product*, and suffers thus
from the same "works on my machine" problem as any other type of software.
Containerization tools offer a great opportunity for reproducibility in
research: instead of just sharing a replication script, authors can now easily
share the right version of the software used to produce these scripts, as well
as the right version of the used libraries by building and sharing a Docker
image (or at least provide the necessary blueprint to enable others to do so, as
we will discuss below). Future researchers looking to replicate the results can
now simple run a container from the provided image (or build an image themselves
if the original authors provided the required blueprints).

Concretely, to build a Docker image, a researcher writes a so-called *Dockerfile*.
Here is an example of a very simple Dockerfile:

```
FROM rocker/r-ver:4.3.0

CMD ["R"]
```

This Dockerfile contains two lines: the first line states which Docker image we
are going to use as a base. Our image will be based on the `rocker/r-ver:4.3.0`
image. The Rocker project is a repository containing many images that ship
different versions of R and packages pre-installed: so the image called
`r-ver:4.3.0` is an image that ships R version 4.3.0. The last line states which
command should run when the user runs a container defined from our image, so in
this case, simply the R interactive prompt. Below is an example of a Dockerfile
that runs an analysis script:

```
FROM rocker/r-ver:4.3.0

RUN R -e "install.packages('dplyr')"

RUN mkdir /home/research_project

RUN mkdir /home/research_project/project_output

RUN mkdir /home/research_project/shared_folder

COPY analyse_data.R /home/research_project/analyse_data.R

RUN cd /home/research_project && R -e "source('analyse_data.R')"

CMD mv /home/research_project/project_output/* /home/research_project/shared_folder/
```

This Dockerfile starts off from the same base image, an image that ships R
version 4.3.0, then it installs the `{dplyr}` package, a popular R package for
data manipulation and it creates three directories:

- `/home/research_project`
- `/home/research_project/project_output`
- `/home/research_project/shared_folder`.

Then, it copies the `analyse_data.R` script, which contains the actual analysis
made for the purposes of the research project, into the Docker image. The
second-to-last line runs the script, and the last line moves the outputs
generated from running the `analyse_data.R` script to a folder called
`shared_folder`. It is important to say that `RUN` statements will be executed
as the image gets built, and `CMD` statements will be executed as a container
runs. Using this Dockerfile, an image called `research_project` can be built
with the following command:

```
docker build -t research_project .
```

This image can then be archived and shared for replication purposes. Future
researchers can then run a container from that image using a command
such as:

```
docker run -d -it --rm --name research_project_container \
  -v /host/machine/shared_folder:/home/research_project/shared_folder:rw \
  research_project
```

The container, called `research_project_container` will execute the `CMD`
statement from the Dockerfile, in other words, move the outputs to the
`shared_folder`. This folder is like a tunnel between the machine that runs the
container and the container itself: by doing this, the outputs generated within
the container can now be accessed from the host's computer.

The image built by the process above is immutable: so as long as users can run
it, the outputs produced will be exactly the same as when the original author
ran the original analysis. However, if the image gets lost, and needs to be
rebuilt, the above Dockerfile will not generate the same image. This is because
the Dockerfile, as it is written above, will download the version of `{dplyr}`
that is current at the time it gets built. So if a user instead builds the image
in 5 years, the version of `{dplyr}` that will get downloaded will not be the
same as the one that was actually used for the original analysis. The version of
R, however, will forever remain at version 4.3.0.

So to ensure that future researchers will download the right versions of
packages that were originally used for the project, the original researcher also
needs to provide a list of packages that were used as well as the packages'
versions. This can be quite tedious if done by hand, but thankfully, there are
ways to generate such lists very easily. The `{renv}` package for the R
programming language provides such a function. Once the project is done, one
simply needs to call:

```{r, eval = FALSE}
renv::init()
renv::hydrate()
renv::snapshot()
```

to generate a file called `renv.lock`. This file contains the R version that was
used to generate it, the list of packages that were used for the project, as
well as their versions and links to download them. This file can be used to
easily install all the required packages in the future by simply running:

```{r, eval = FALSE}
renv::restore()
```

A researcher can thus add the following steps in the Dockerfile to download the
right packages when building the image:

```
COPY renv.lock /home/research_project/renv.lock

RUN R -e "setwd('/home/research_project);renv::init();renv::restore()"
```

However, what should researchers that want to replicate a past study do if the
original researcher did not provide a Dockerfile nor an `renv.lock` file? This
is exactly the challenge that we were facing when trying to replicate the
results of @davenport2022multiracial. We needed to find a way to first install
the right version of R, then the right version of the packages that they used,
run their original script, and then repeat this procedure but this time on a
recent version of R.

In order to achieve this, we used a Docker image provided by the [R Installation
Manager](https://github.com/r-lib/rig)^[https://github.com/r-lib/rig] project.
This Docker image includes a tool, called `rig`, that makes it easy to switch R
versions, so we used it to first define an image that would use R version 3.5.0
by default as a base. Here are the three commands from the Dockerfile to achieve
this:

```
FROM rhub/rig:latest

RUN rig install 3.5.0

RUN rig default 3.5.0
```

Then, we had to install the packages that the original authors used to perform
their analysis. We had to make some assumptions: since we only had the list of
used packages, but not their exact versions, we assumed that the required
packages were installed one year before the paper was published, so sometime in
May 2019. With this assumption, we then used the [Posit Package
Manager](https://packagemanager.posit.co/client/#/repos/2/overview)^[https://packagemanager.posit.co/client/#/repos/2/overview],
which provides snapshots of CRAN that can be used to install R packages as they
were on a given date. We thus configured R to download packages from the
snapshot taken on May 16th, 2019. Then, the original replication script gets
executed at image build time and we can obtain the outputs from running a
container from this image definition. With this setup, it was very simple to
only switch R versions and re-executed everything. We simply had to switch the
commands from the Dockerfile:

```
FROM rhub/rig:latest

RUN rig install 4.2.0

RUN rig default 4.2.0
```

Everything else: package versions and operating system that the replication
script runs on, stayed the same.

With this setup, we were thus able to run the original analysis on an
environment that was as close as possible to the original environment used by
@davenport2022multiracial. However, it is impossible to regenerate the exact
environment now. As stated, we had to make an assumption on the date the
packages were downloaded, but the packages use by the original authors might
have been much older. Another likely difference is that the operating system
used inside Docker is the Ubuntu Linux distribution. While Ubuntu is a popular
Linux distribution, it is much more likely that the original authors used either
Windows or macOS to perform their analysis. In most cases, the operating system
does not have an impact on results, but there have been replication studies that
failed because of this, as in @neupane2019. Thankfully, mismatch of the
operating system does not seem to be an issue here.

It should also be noted that strictly speaking, using Docker is not completely
reproducible either: this is because the underlying base image, upon which
we build our complete analysis, also changes through time. In the examples above
the image we used, `rhub/rig:latest` is based on Ubuntu 22.04, which is, at of
writing, the latest long-term support release of the Ubuntu operating system. This
version of Ubuntu thus still gets updated: so when building the image for our project,
the underlying base image will be different today than from 6 months from now. And
when the next Ubuntu long-term support will be released, in April 2024, then 
`rhub/rig:latest` will this time be based upon this new release of Ubuntu. So building
in May 2024 will yield a different image than now. In order to avoid this, it is
possible to use a so-called *digest* which ensure that exactly the same image will
forever be used as a base. For the purposes of this paper we used the following
image: `rhub/rig@sha256:0667321839be0c2ade5c14fd8c8bbb311a843ddf690081c5d0dd80dcc98bd6c6`.

Finally, to ensure completeness, instead of using the Posit Package Manager with a fixed date,
we could have used the `{groundhog}` package by @simonsohn2023. This package makes it possible to
download packages as of a certain date as well, and it does not require users
to change any of R's default configurations.

### Reproducibility isn't just version and package management — using {targets} to improve readibility and reproducibility

Reproducibility should go beyond providing the right versions of the analysis
software used. Another import aspect is *readability*, which is difficult to
quantify. There are however some approaches that we suggest researchers could
employ to improve the readability of their code and thus increase the
probability of a successful replication.

Most research papers and studies' computer code is written as one, or several,
scripts that must be run in a certain order. These script-based workflows
usually grow very large and become chaotic. Documentation must be written
alongside the scripts to explain how they work and in which order they're
supposed to be executed, and this documentation then has to be maintained and
updated as the scripts evolve. When some parts of the code gets changed in one
script, the researcher has to remember which parts of the other scripts get
impacted and either only run the relevant, now outdated, parts, or re-run the
whole project which can be quite time-consuming. Here again, it helps to view a
researcher paper (and in particular its computer code) as a piece of software.
Software engineers are faced with the exact same problem but solved it many
decades ago using so-called build automation tools. These build automation tools
allow one to describe how a particular piece of software should get built. Other
software engineers that wish to build that piece of software thus only need to
execute the build automation tool which will take care of executing the right
steps in the right order.

The solution we propose is for researchers to use build automation tools. For
the R programming language, a very popular build automation tool is provided
through the `{targets}` package by @landau2021. Using `{targets}` has many
benefits:

- `{targets}` keeps track of all the inter-dependencies between the different pieces of the entire codebase. If some part gets changed, `{targets}` *knows* which other parts are affected and only re-executes these;
- another consequence of this is that `{targets}` also knows which parts of the codebase are independent from each other and can thus be safely executed in parallel. This can lead to tremendous execution speed gains;
- `{targets}` works by having the user define the computations as a pipeline. This pipeline is in essence the composition of many pure functions, which increases the readibility of the project.

As an illustration of this, we rewrote parts of the original study as a
`{targets}` pipeline, by re-using the original code from the authors and
rewriting only what was needed to "port" it to `{targets}`. A `{targets}`
pipeline is defined by at least one file called `_targets.R`. Within this
file, a list of `targets` objects is defined. For example, here is the 
beginning of our `_targets.R` that we had to write for our purposes
(not shown are the calls to load required libraries and helper functions):

```
list(
  tar_target(
    data_path,
    "/home/r_and_p/data/multiracial.sav"
  ),
  tar_target(
    multiracial_raw,
    read_sav(data_path)
  ),
  tar_target(
    multiracial_1,
    basic_clean_1(multiracial_raw)
  ),
  tar_target(
    multiracial_2,
    closeness_2(multiracial_1)
  ),
```

The first target, `data_path` simply defines the path to the data set from the 
original study. The second target, `multiracial_raw` is the data loaded into
memory. This second target is defined as the output of the `read_sav(data_path)`
command. Because this second target uses the first target as an input, `{targets}`
is able to detect that these targets are linked. The third target is 
`multiracial_1`, which is defined as the output of a function called `basic_clean_1()`.
This is a function that we defined into another script called `functions.R`. This 
script contains all the code from the original study that was written to clean 
the data. The only difference is that we have re-defined every piece of code into
a function. For example, this is what `basic_clean_1()` looks like:

```
basic_clean_1 <- function(dataset){
  dataset %>%
      clean_names() %>%
      mutate(across(where(is.labelled),
                    as_factor)) %>%
      # Exclude South Asians
      filter((is.na(q24_2) | q24_2 == "No") &
             (is.na(q24_3) | q24_3 == "No")) %>%
      mutate(educ3 = case_when(
               educ %in% c("No HS", "High school graduate") ~
                 "Less than or equal HS",

               educ == "Some college" ~
                 "Some college",

               educ %in% c("2-year", "4-year", "Post-grad")  ~
                 "College Grad",
               TRUE ~ NA_character_
             )) %>%
    mutate(age = 2015 - as.numeric(as.character(birthyr))) %>%
    mutate(income = if_else(faminc == "Prefer not to say", NA, faminc),
           income = fct_lump_min(income, 100,
                                 other_level = ">150,000")) %>%
    mutate(south = ifelse(
             inputstate %in% c("Alabama", "Arkansas", "Florida",
                               "Georgia", "Kentucky", "Louisiana",
                               "Mississippi", "North Carolina",
                               "South Carolina", "Tennessee", "Texas",
                               "Virginia", "West Virginia"),
             1, 0
           ))
}
```

and here is the equivalent, original code, as a series of imperative calls:

```
colnames(df) <- tolower(colnames(df))

# Exclude South Asians
df <- subset(df, (is.na(df$q24_2) | q24_2 == "No") &
               (is.na(df$q24_3) | q24_3 == "No"))

#############
## RECODES ##
#############

## RACIAL GROUP ##
df$race <- df$q_11rand
levels(df$race) <- c("WHITE-BLACK", "WHITE-ASIAN", "WHITE", "BLACK", "ASIAN")
df$race <- relevel(df$race, ref = "WHITE")
table(df$race)


## EDUCATION ##
df$educ3 <- recode(as.numeric(df$educ), "2=1; 3=2; 4=2; 5=3; 6=3")
df$educ3 <- factor(df$educ3, labels = c("Less than or equal HS", "Some College", "College Grad"))

## AGE ##
df$age <- 2015 - as.numeric(as.character(df$birthyr))


## INCOME ##
df$income <- ifelse(df$faminc == "Prefer not to say", NA, as.numeric(df$faminc))
df$income <- ifelse(df$income > 11, 12, df$income)
summary(df$income)


## REGION ##
southern_states <- c("Alabama", "Arkansas", "Florida", "Georgia", "Kentucky",
                     "Louisiana", "Mississippi", "North Carolina", "South Carolina",
                     "Tennessee", "Texas", "Virginia", "West Virginia")
df$south <- ifelse(df$inputstate %in% southern_states, 1, 0)

```

The original approach, which uses an imperative style of programming, keeps
changing the `df` object, which is the original SPSS data loaded into memory.
The issue with this approach is that it becomes very difficult to read and
follow as the number of operations grows. It is also impossible to easily test
imperative code. Functions on the other hand, are very easy to test, and very
easy to chain together. Users interested in implementation details can look into
the source code of the function to understand how it works. If not, reading
`basic_clean_1(dataset)` is enough to convey that some basic cleaning operations
were applied to `dataset`. There is no need to burden the user with too much
code all at once.

The fourth target uses the output of the third target as an input and is defined
as the output of the `closeness_2()` to function. Here again, readers that are
interested into what `closeness_2()` does can look at its source code. If not,
they can move on.

While defining the series of operations as a list of targets improves readability,
it is sometimes much easier to look at a visual representation of the pipeline.
This is possible using a command from the `{targets}` package called `tar_visnetwork()`
Here is the visual representation of the pipeline, before actually running it:

```{r fig3dagbefore, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure3_dag_before.png", error = FALSE)
```

At a glance, it is possible to determine the following:

- a single dataset is being manipulated by several functions;
- each new instance of the dataset is the output of one, or several functions;
- there are two functions that were defined that are not being used at all (bottom left).

This is actually an interactive visualisation that opens in the users web-browser,
so it is possible to zoom in and read the labels of the different objects.

To run the pipeline, and compute all the targets, users must execute `targets::tar_make()`.
This is how the pipeline looks like once it's been executed:

```{r fig4dagbefore, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure4_dag_after.png", error = FALSE)
```

Each target is now updated, in other words, it has been computed. Any of the
intermediary targets, as well as the very last target, `table_1`, can now be
loaded into the R session in which the pipeline was executed using
`targets::tar_load(table_1)`. Every target gets automatically saved into a
folder that will appear after the first execution of the pipeline, called
`_targets`. This folder contains all the targets, which means that even if I
execute the pipeline in a completely new session the following day, the whole
pipeline will get skipped, because all the targets have already been computed.
This can lead to tremendous time savings. Also, if users do change some code,
somewhere in the project, `{targets}` will automatically detect which line of
code was changed, and what is the impact on the rest of the pipeline. For
example, here we changed some code in one of the functions and visualise the
pipeline:

```{r fig5dagood, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure5_dag_ood.png", error = FALSE)
```

One function was changed, which impacted another function, which in turn impacted
one of the intermediary datasets. Because this intermediary dataset was impacted,
every other dataset down the pipeline will have to be recomputed up to the final
target. Let's zoom in a bit:

```{r fig6dazoom, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure6_dag_zoom.png", error = FALSE)
```

The `oneto0()` function was changed (which is a piece of code that was also
defined as a function in the original study), but because `closeness_2()` uses
this function internally, it was thus impacted by that change. Finally, this
means that the output of `closeness_2()` (`multiracial_2`) is also impacted, and
thus needs to be recomputed, impacting every other target that uses `multiracial_2`
as an input directly or indirectly.

In essence, `{targets}` forces its users to write small, well written functions
and then chain them one after the next in order to generate an output.
Visualising the execution of the code, and the impact that changing some pieces
of code have on the whole pipeline helps readability immensely. Future users can
start by looking at the `_targets.R` file, and then visualise the pipeline and
get a high-level understanding of the whole computational process. If they want
to read the technical implementation details of one or several functions, they
can do so by simply going to the source code of the relevant function.

One final important note to improve reproducibility of a project: the output of
scientific research is more often than not a PDF document. We highly recommend
using literate programming (@knuth1984literate) coupled with `{targets}` to
produce articles. In practice this means that the paper itself will get
generated by `{targets}` as the final output of the whole pipeline. A fairly recent,
but already very popular engine for literate programming that also supports
\LaTeX notation is Quarto (@Allaire_Quarto_2022).

\newpage

## Conclusion

- Docker for R and OS
- renv or groundhog or PPPM for packages
- targets for a pipeline and Quarto for Literate programming
- importantly: what’s the very minimum you should do to make your project reproducible? the bare minimum
- mention Nix as an alternative Docker plus packages because it's always a good idea to have alternatives

\newpage

## References
