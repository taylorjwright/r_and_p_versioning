---
bibliography: rodrigues_wright_versioning.bib
format: 
  pdf:
    number-sections: true
geometry: margin = 1.0in
font-size: 11pt
mainfont: cochineal
sansfont: Helvetica
header-includes:
  \usepackage{booktabs}
  \usepackage{setspace}
  \doublespacing
  \usepackage[left]{lineno}
  \linenumbers
---

\date{\today}
\title{\textbf{Replication materials are great but software versioning still poses a problem for open science}}
\author{Taylor J. Wright\thanks{Brock University, \href{mailto:twright3@brocku.ca}{twright3@brocku.ca}} \and Bruno Rodrigues \thanks{Ministry of Higher Education and Research, Luxembourg}
}

```{r, echo=FALSE, include=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(ggplot2)
library(flextable)
```

\maketitle

\newpage

## Introduction

In recent years, the ''replication crisis'' in the social sciences has become mainstream, even featured in the popular press. Concerns over how well studies hold up to replication in other settings has lead to large scale initiatives to document how trustworthy the existing stock of evidence is (cite  Camerer and Nosek). While the replicability of studies in important, there has been a tandem movement discussing how reproducible research results are. There are various definitions, but for broadly speaking replication is in some sense re-testing a hypothesis while changing an element of previous research (e.g. the sample, or the estimating equation) whereas reproducibility is following a study's protocol exactly and obtaining the results presented in the study (insert a citation with some definitions here, perhaps Lars' blog or Michael Clemens JES paper?). In our view, reproducibility is an insufficient but necessary condition for replication---that is, it does not make sense to spend resources on replciation if research is not reproducible.   

These concerns about the reproducibility and replicability of social science research have prompted a push for journals to require the publication of research materials that accompany academic research (see, for example, the 2014 Joint Editors Transparency Statement which was signed by editors of 27 leading political science journals: \url{https://www.dartstatement.org/2014-journal-editors-statement-jets}). Specifically, the provision of underlying data and scripts used for data preparation and analysis. However, even if these materials are provided (and even when in place these policies do not have perfect compliance (@philip2010report, @stockemer2018data)), the regular software updates and new version releases can result in the replication materials failing to faithfully reproduce the authors' results or even run at all (@simonsohn95GroundhogAddressing2021 presents several examples of \texttt{R} changes that could break scripts). 


In this paper we ppresent a case study of an article published in Journal of Politics in January 2022 titled, ``Multiracial Identity and Political Preferences'', @davenport2022multiracial, that details that replication challenges arising from changes in the statistical software \texttt{R}. We were unable to reproduce the authors' results using either the current version of \texttt{R}, or the version that the authors indicate they used. The lack of reproducibility arose due to a change in the defaults used by base \texttt{R} when generating random numbers starting in version 3.6.0. 

We contribute to the existing literature...

The rest of the article proceeds as follows: Section 2 walks through the reproducibility issues in @davenport2022multiracial; Section 3 discusses currently available tools and best practices (e.g. Docker and \texttt{R} packages such as \texttt{renv}, \texttt{groundhog}) for ensuring that replication materials continue to faithfully reproduce research results, despite post-publication changes in the tools used; and Section 4 concludes.

\newpage

## Reproduction

### Illustrating the issue with software versioning 
A key thing here is that I don't think we want to be too hostile sounding towards these authors, they had readme files and reproducibility materials available. It's just that manual entry human-error hobgoblins got them with the R versioning.

- Brief discussion of authors' paper and context
- The results of their code using stated software version in documentation (just screenshot right now)
```{r fig1screenshot, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure1_screenshot.png", error = FALSE)
```

```{r fig2screenshot, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure2_screenshot.png", error = FALSE)
```
- The results of their code using later software version (post 3.6.0)

```{r fig1post, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure1_postchange.png", error = FALSE)
```

```{r fig2post, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure2_postchange.png", error = FALSE)
```
- Results of their code using earlier software version (pre 3.6.0)
```{r fig1pre, echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figures/figure1_prechange.png", error = FALSE)
```

```{r fig2pre, echo=FALSE, out.width = '100%'}
# need to tweak axes to match authors, CI cap is truncated
knitr::include_graphics("./figures/figure2_prechange.png", error = FALSE)
```

The authors note that they use R version 3.6.2. Issue seems to be the weights the authors use are non-integer and Zelig uses sample() in that case which following changes to base R yields different results in 3.6.x and 3.5.x (see http://docs.zeligproject.org/articles/weights.html). Prior to R 3.6.x RNGkind(sample.kind = "Rounding") was the default behaviour but after 3.6.0 the sample function's new default behaviour is RNGkind(sample.kind = "Rejection") (see https://blog.revolutionanalytics.com/2019/05/whats-new-in-r-360.html). 

Soemthing about how there are ways to ensure that the estimates are consistent (changing the weights to integers or setting the RNGkind to be backwards compatible) but documenting the correct version of R used in the analysis is probably the easiest way. Segue into...

\newpage

## Discussion

### The problem is the seed?

The table below shows the quantiles of the means obtained from 100 runs with 
100 different random seeds on R version 3.5 for the `m2m` models. We should
check where the coefficients from the original paper fall in that distribution,
and see if coefficients should vary so much simply from changing the seed?

```{r echo = FALSE, message = FALSE}
simulations_m2m_paths <- list.files("../docker/version_3.x_many_seeds/original/shared_folder/", pattern = "m2m.*3_5.*\\d{1,}.rds", full.names = TRUE)

simulations_m2m <- tibble(paths = simulations_m2m_paths) %>%
  mutate(datasets = map(paths, readRDS),
         paths = str_extract(paths, "\\d{1,}.rds")) %>%
  unnest(cols = c(datasets))

simulations_m2m %>%
  group_by(race, model) %>%
  summarise(
    q_005_mean = quantile(mean, probs = 0.05),
    q_20_mean = quantile(mean, probs = 0.2),
    q_40_mean = quantile(mean, probs = 0.4),
    q_50_mean = quantile(mean, probs = 0.5),
    q_60_mean = quantile(mean, probs = 0.6),
    q_80_mean = quantile(mean, probs = 0.8),
    q_95_mean = quantile(mean, probs = 0.95)
  ) %>%
  flextable()
```

The table below shows the quantiles of the means obtained from 100 runs with 
100 different random seeds on R version 3.5 for the `m2w` models:

```{r echo = FALSE, message = FALSE}
simulations_m2w_paths <- list.files("../docker/version_3.x_many_seeds/original/shared_folder/", pattern = "m2w.*3_5.*\\d{1,}.rds", full.names = TRUE)

simulations_m2w <- tibble(paths = simulations_m2w_paths) %>%
  mutate(datasets = map(paths, readRDS),
         paths = str_extract(paths, "\\d{1,}.rds")) %>%
  unnest(cols = c(datasets))

simulations_m2w %>%
  group_by(race, model) %>%
  summarise(
    q_005_mean = quantile(mean, probs = 0.05),
    q_20_mean = quantile(mean, probs = 0.2),
    q_40_mean = quantile(mean, probs = 0.4),
    q_50_mean = quantile(mean, probs = 0.5),
    q_60_mean = quantile(mean, probs = 0.6),
    q_80_mean = quantile(mean, probs = 0.8),
    q_95_mean = quantile(mean, probs = 0.95)
  ) %>%
  flextable()

```

### Rebuilding the original development environment using Docker

Replicating results from past studies is quite challenging, for many reasons.
This articles focuses on one of these reasons: results cannot be reproduced
because of changes introduced in more recent versions of the software used for
analysis, despite the availability of both data and replication scripts.

To replicate the results from the original study and to pinpoint the impact of
the change introduced in R 3.6.0, we chose to use Docker. Docker is a
containerization tool which enables one to build so-called *images*. These
images contain a software product alongside its dependencies and even pieces of
a Linux operating system. To use the software product, customers in turn only
need to be able to run Docker *containers* from the image definition.
Containerization tools such as Docker solved the "works on my machine" problem:
this problem arises when software that works well on the development machine of
the developer fails to run successfully on a customer's machine. This usually
happens because the customer's computer does not have the necessary dependencies
to run the software (which are traditionally not shipped alongside the software
product).

A research project can also be seen as a *software product*, and suffers thus
from the same "works on my machine" problem as any other type of software.
Containerization tools offer a great opportunity for reproducibility in
research: instead of just sharing a replication script, authors can now easily
share the right version of the software used to produce these scripts, as well
as the right version of the used libraries by building and sharing a Docker
image (or at least provide the necessary blueprint to enable others to do so, as
we will discuss below). Future researchers looking to replicate the results can
now simple run a container from the provided image (or build an image themselves
if the original authors provided the required blueprints).

Concretely, to build a Docker image, a researcher writes a so-called *Dockerfile*.
Here is an example of a very simple Dockerfile:

```
FROM rocker/r-ver:4.3.0

CMD ["R"]
```

This Dockerfile contains two lines: the first line states which Docker image we
are going to use as a base. Our image will be based on the 'rocker/r-ver:4.3.0'
image. The Rocker project is a repository containing many images that ship
different versions of R and packages pre-installed: so the image called
`r-ver:4.3.0` is an image that ships R version 4.3.0. The last line states which
command should run when the user runs a container defined from our image, so in
this case, simply the R interactive prompt. Below is an example of a Dockerfile
that runs an analysis script:

```
FROM rocker/r-ver:4.3.0

RUN R -e "install.packages('dplyr')"

RUN mkdir /home/research_project

RUN mkdir /home/research_project/project_output

RUN mkdir /home/research_project/shared_folder

COPY analyse_data.R /home/research_project/analyse_data.R

RUN cd /home/research_project && R -e "source('analyse_data.R')"

CMD mv /home/research_project/project_output/* /home/research_project/shared_folder/
```

This Dockerfile starts off from the same base image, an image that ships R
version 4.3.0, than it install the `{dplyr}` package, a popular R package for
data manipulation and it creates three directories:

- `/home/research_project`
- `/home/research_project/project_output`
- `/home/research_project/shared_folder`.

Then, it copies the `analyse_data.R` script, which contains the actual analysis
made for the purposes of the research project, into the Docker image. The
second-to-last line runs the script, and the last line moves the outputs
generated from running the `analyse_data.R` script to a folder called
`shared_folder`. It is important to say that `RUN` statements will be executed
as the image gets built, and `CMD` statements will be executed as a container
runs. Using this Dockerfile, an image called `research_project` can be built
with the following command:

```
docker build -t research_project .
```

This image can then be archived and shared for replication purposes. Future
researchers can then run a container from that image using a command
such as:

```
docker run -d -it --rm --name research_project_container \
  -v /host/machine/shared_folder:/home/research_project/shared_folder:rw \
  research_project
```

The container, called `research_project_container` will execute the `CMD`
statement from the Dockerfile, in other words, move the outputs to the
`shared_folder`. This folder is like a door between the machine that
runs the container and the container: by doing this, the outputs generated
within the container can now be accessed from the host's computer.

The image built by the process above is immutable: so as long as users can run
it, the outputs produced will be exactly the same as when the original author
ran the original analysis. However, if the image gets lost, and needs to be
rebuilt, the above Dockerfile will not generate the same image. This is because
the Dockerfile, as it is written above, will download the version of `{dplyr}`
that is current at the time it gets built. So if a user instead builds the image
in 5 years, the version of `{dplyr}` that will get downloaded. The version of R,
however, will forever remain at version 4.3.0.

So to ensure that future researchers will download the right versions of packages
that were originally used for the project, the original researcher also needs
to provide a list of packages and their versions. This can be quite tedious
if done by hand, but thankfully, there are ways to generate such lists very
easily. The `{renv}` package for the R programming language provides such a function.
Once the project is done, one simply needs to call:

```
renv::init()
renv::hydrate()
renv::snapshot()
```

to generate a file called `renv.lock`. This file contains the R version that was used
to generate it, the list of packages that were used for the project, as well as their
versions and links to download them. This file can be used to easily install all the
required packages in the future by simply running:

```
renv::restore()
```

A researcher can thus add the following steps in the Dockerfile to download the right
packages when building the image:

```
COPY renv.lock /home/research_project/renv.lock

RUN R -e "setwd('/home/research_project);renv::init();renv::restore()"
```

However, what should researchers that want to replicate a past study do if the
original researcher did not provide a Dockerfile nor an `renv.lock` file? This
is exactly the challenge that we were facing when trying to replicate the
results of @davenport2022multiracial. We needed to find a way to first, install
the right version of R, then the right version of the packages that they used, run
their original script, and then repeat this procedure but this time on a recent
version of R.



### Reproducibility isn't just version and package management — using {targets} to improve readibility and reproducibility


\newpage

## Conclusion

\newpage

## References
