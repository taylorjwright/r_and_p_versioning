{
  "hash": "c335ae8afa6bfd72964bd2c8c1b01acb",
  "result": {
    "engine": "knitr",
    "markdown": "---\nbibliography: rodrigues_wright_versioning.bib\nformat: \n  pdf:\n    number-sections: true\n    shift-heading-level-by: -1\ngeometry: margin = 1.0in\nfont-size: 11pt\n# mainfont: cochineal\n# sansfont: \"TeX Gyre Heros\"\nheader-includes:\n  \\usepackage{booktabs}\n  \\usepackage{setspace}\n  \\doublespacing\n  #\\usepackage[left]{lineno}\n  #\\linenumbers\n---\n\n\n\n\\date{\\today}\n\\title{\\textbf{Replication materials are great but software versioning still poses a problem for open science}}\n\\author{Taylor J. Wright\\thanks{Brock University, \\href{mailto:twright3@brocku.ca}{twright3@brocku.ca}} \\and Bruno Rodrigues \\thanks{Ministry of Research and Higher education, Luxembourg}\n}\n\n\\maketitle\n\\singlespacing\n\\begin{abstract}\nThe growing concern regarding reproducibility and replicability of social science \nresults has powered the adoption of open data and code requirements at journals \nand norms among researchers. However, even when these norms and requirements are \nfollowed, changes to the software used in data cleaning and analysis can render \npapers non-reproducible. This paper details the challenges of reproducibility in \nthe face of software updates. We present a case study of a published article whose \nresults are no longer reproducible due to changes in the software used. We then \ndiscuss the tools and techniques researchers can use to ensure that their research \nremains reproducible despite changes in the software used.\n\\end{abstract}\n\n\\doublespacing\n\n\n\n\n\n\n\n\\newpage\n\n## Introduction {#sec-introduction}\n\nBeginning in the early 2010s with psychology and quickly spilling over into\npolitical science and economics on the backs of high-profile cases of fraud\n[@broockman2015irregularities] and errors [@herndon2014does], the \\`\\`replication\ncrisis\\\" in the social sciences has become mainstream, even finding its way into\nthe popular press [@aschwanden2015; @gelman2018]. Concerns over how well\npublished results hold up to replication in other settings has led to large\nscale initiatives to document how trustworthy the existing stock of evidence is\n[e.g. @camerer2016evaluating; @chang2022replicable; and Open Science Collaboration -@open2015estimating].\nWhile the *replicability* of studies has received much attention, there has been\na tandem movement discussing the importance of and need for research\n*reproducibility*. While the exact scope and definition is subject to ongoing\ndebate, replication is, broadly speaking, re-testing a hypothesis while changing\nan element of previous research (e.g. the sample or estimating equation) whereas\nreproducibility (sometimes referred to as verification or computational\nreproducibility) is following a study's protocol or methods exactly and, using the \noriginal data, obtaining the results presented in the study.^[For further discussion on\ndefinitions, scopes, and types of replication and reproducibility see, for\nexample, @clemens2017meaning, @derksen2022kinds, @hamermesh2007replication, and\n@nosek2020replication.]\n\nIn our view, reproducibility is an insufficient but necessary condition for\nreplication---that is, work cannot be replicable if it is not reproducible and\nit likely does not make sense to spend resources on replication if research is\nnot in the first place reproducible.\n\nThese concerns about the reproducibility (and replicability) of social science\nresearch have prompted a push, often referred to as Data Access and Research\nTransparency (DA-RT) in political science following @lupia2014openness, for\njournals to require the publication of research materials that accompany\nacademic research.^[See, for example, the 2014 Joint Editors Transparency\nStatement which was signed by editors of 27 leading political science journals:\n\\url{https://www.dartstatement.org/2014-journal-editors-statement-jets}]\nSpecifically, the provision and publication of underlying data and code used for\ndata preparation and analysis. This push has seen some success, with a growing\nnumber journals now requiring the provision of replication materials as a\ncondition of publication. @rainey2024data collect data availability policies between\nJanuary and April of 2023 for \\`\\`Political Science\\\" and \\`\\`International Relations\\\" \njournals in Social Science Citation Index and APSA journals, finding that 20% require \ndata to be shared and 65% encourage (but do not require) it. Some journals, such as \nthe American Economic Review(AER) or American Journal of Political Science (AJPS), even \nhave verification policies that require authors to upload their replication materials and \nhave their results verified by another team (either the journal's replication team (AER) or\na third party (AJPS)) before publication. However, even if these materials\nare provided, and even when in place these policies do not have perfect\ncompliance [@philip2010report; @stockemer2018data], regular software updates and\nnew version releases can result in the replication materials failing to\nfaithfully reproduce the authors' results or even run at all.\n^[@simonsohn95GroundhogAddressing2021 presents several examples of \\texttt{R}\nchanges that could break scripts.]\n\nIn this paper, we present a case study of an article published in Journal of\nPolitics in January 2022 titled, \\`\\`Multiracial Identity and Political\nPreferences\\\", [@davenport2022multiracial], that details replication\nchallenges arising from changes in the statistical software \\texttt{R}. We were\nunable to reproduce the authors' results using either the current version of\n\\texttt{R}, or the version that the authors indicate they used. The lack of\nreproducibility arose due to a change in the defaults used by base \\texttt{R}\nwhen generating  numbers starting in version 3.6.0.\n\nWe contribute to the existing literature in three ways. \nFirst, we add to the discussion about the importance of the availability of \nreplication materials. Existing research documents that the availability of \nreplication materials is associated with a higher citation count \n[@piwowar2007sharing; @piwowar2013data] and with a higher likelihood of \nreplication [@philip2010report]. Much of this research focuses on the provision \nof data and code as a necessary condition for facilitating replication and \nrebuilding trust in science [@dafoe2014science; @lupia2014data]. We contribute \nto this literature by emphasizing \nthat, while we agree that data and code availability are necessary, their \nexistence cannot guarantee reproducibility. This brings us to our second \ncontribution: we expand on the discussion of problems raised by software and \npackage versioning for reproducibility by providing a walkthrough of the problem\nusing a concrete example. Lastly, we further the discussion of tools and best \npractices for ensuring reproducibility. Specifically, we provide a step-by-step \nguide to using Docker and the \\texttt{renv} \\texttt{R} package to ensure the reproducibility of \nresearch.\n\nThe rest of the article proceeds as follows: Section 2 walks through the\nreproducibility issues in @davenport2022multiracial; Section 3 discusses\ncurrently available tools and best practices (e.g. Docker and \\texttt{R}\npackages such as \\texttt{renv}, \\texttt{groundhog}) for ensuring that\nreplication materials continue to faithfully reproduce research results, despite\npost-publication changes in the tools used; and Section 4 concludes.\n\n\\newpage\n\n## Reproduction {#sec-reproduction}\n\n### Illustrating the issue with software versioning\n\n@davenport2022multiracial examine the political ideology and policy preferences \nof multiracial Americans (those who identify with more than one racial group). \nThis section of Americans is growing rapidly, but little is known about their\npolitics. The authors survey White-Asians and White-Blacks from YouGov's Internet \npanel and compare them to similarly surveyed White, Black, and Asian monoracial \nrespondents, looking for differences in political ideology, party affiliation,\npositions about cultural issues, social welfare, and police treatment across \nracial groups.\n\n::: {layout=\"[[1], [1], [1]]\"}\n![Figure 1 from @davenport2022multiracial. Multiracial attitudes relative to monoracial minorities. A, White-Asian; B, White-Black. Higher values are more liberal attitudes. \\label{dfifig1}](./figures/figure1_screenshot.png)\n\n![Figure 1 from @davenport2022multiracial when using a later software version (R post 3.6.0). Multiracial attitudes relative to monoracial minorities. A, White-Asian; B, White-Black. Higher values are more liberal attitudes. \\label{postfig1}](./figures/figure1_postchange.png)\n\n![Figure 1 from @davenport2022multiracial when using an earlier software version (R pre 3.6.0). Multiracial attitudes relative to monoracial minorities. A, White-Asian; B, White-Black. Higher values are more liberal attitudes. \\label{prefig1}](./figures/figure1_prechange.png)\n:::\n\n\nFigure \\ref{dfifig1} and presents Figure 1 from @davenport2022multiracial \nwhich contains the main results. Panel A corresponds to results for White-Asian \nrespondents while panel B corresponds to White-Black respondents. The x-axis \nreflects the differences in a given multiracial group's attitudes relative to the \ncorresponding monoracial group (e.g. White-Asian relative to Asians) with higher \nvalues meaning more liberal attitudes. This figure indicates that White-Asians have\nmore liberal attitudes on cultural issues than Asians while White-Blacks have more \nliberal attitudes on cultural issues than Blacks but views regarding social welfare \nand police perception appear to be comparable between White-Asians and Asians and \nbetween White-Blacks and Blacks. \n\nImportantly, the authors provide replication materials at the Journal of Politics \nDataverse, including the code used to conduct their analysis as well as the data \nitself. The code files contain comments noting that the analysis \\`\\`Requires \\texttt{R} \nversion 3.6.2 (2019-12-12)\\\". This allows us to recreate the figures.\n\nFigure \\ref{postfig1} presents Figure 1 from @davenport2022multiracial when using \na later software version (specifically a version of \\texttt{R} post 3.6.0 such as \nthe version 3.6.2 that the authors note in their code files). In Panel A, White-Asians \nare now more liberal than Asians and Whites on all issues. In Panel B White-Blacks have \nmore liberal views on cultural issues than Blacks (in terms of point estimates though\nthe confidence interval now contains 0) and are now more liberal regarding police perceptions. \n\nFigure \\ref{prefig1} provides Figure 1 @davenport2022multiracial when using an \nearlier software version (specifically a version of \\texttt{R} pre 3.6.0). \nThese are otherwise the same as those presented in the published article.\n\n::: {layout=\"[[1], [1], [1]]\"}\n![Figure 2 from @davenport2022multiracial. Multiracial attitudes relative to monoracial Whites. A, White-Asian; B, White-Black. Higher values are more liberal attitudes. \\label{dfifig2}](./figures/figure2_screenshot.png)\n\n![Figure 2 from @davenport2022multiracial when using a later software version (R post 3.6.0). Multiracial attitudes relative to monoracial Whites. A, White-Asian; B, White-Black. Higher values are more liberal attitudes. \\label{postfig2}](./figures/figure2_postchange.png)\n\n![Figure 2 from @davenport2022multiracial when using an earlier software version (R pre 3.6.0). Multiracial attitudes relative to monoracial Whites. A, White-Asian; B, White-Black. Higher values are more liberal attitudes. \\label{prefig2}](./figures/figure2_prechange.png)\n:::\n\nFigure \\ref{dfifig2} presents Figure 2 from @davenport2022multiracial. Again, \nPanel A corresponds to results for White-Asian respondents while panel B corresponds \nto White-Black respondents. The x-axis reflects a similar difference but relative\nto monoracial Whites (e.g. White-Asian relative to Whites). Figure 2 Panel A \nsuggests that White-Asians are more liberal than Whites when it comes to cultural \nissues and police perceptions but similarly liberal with views on social welfare. \nFigure 2 Panel B indicates that White-Blacks are similarly liberal to Whites on \ncultural issues but are more liberal on issues of social welfare and police perceptions.\n\nFigure \\ref{postfig2} presents Figure 2 from @davenport2022multiracial when using \na later software version (specifically a version of \\texttt{R} post 3.6.0 such as \nthe version 3.6.2 that the authors note in their code files). In Panel A, \nWhite-Asians are now more liberal than Asians and Whites on all issues. In Panel B \nof Figure \\ref{postfig2} the point estimates appear very similar though there is \nan issue computing confidence intervals.\n\nFigure \\ref{prefig2} provides Figure 2 from @davenport2022multiracial when using \nan earlier software version (specifically a version of \\texttt{R} pre 3.6.0). \nMinor issue with the confidence interval in Panel B aside, these are also otherwise \nthe same as those presented in the published article. \n\nThe issue seems to be that the weights the authors use are non-integer and the \nmain function used in their analysis, \\texttt{Zelig()}, uses \\texttt{sample()} in cases with \nnon-integer values (see\n[http://docs.zeligproject.org/articles/weights.html](http://docs.zeligproject.org/articles/weights.html)).\nFollowing changes to base \\texttt{R}  in 3.6.x the sample() function, and thus now \\texttt{Zelig()}, yields different results in \\texttt{R} versions later than 3.5.x. Before \\texttt{R} 3.6.0 `RNGkind(sample.kind = \"Rounding\")` was the default behaviour but after 3.6.0 the sample function's new default \nbehaviour is `RNGkind(sample.kind = \"Rejection\")`.^[See this announcement about \\texttt{R} 3.6.0 for more details: \n[https://blog.revolutionanalytics.com/2019/05/whats-new-in-r-360.html](https://blog.revolutionanalytics.com/2019/05/whats-new-in-r-360.html)] \nWhy was this change made? According to the `RNGkind` documentation, \n\\`\\`sample.kind\\\" can be \\`\\`Rounding\\\" or \\`\\`Rejection\\\", or partial matches to these. \nThe former was the default in versions prior to 3.6.0: it made sample noticeably\nnon-uniform on large populations, and should only be used for reproduction of old \nresults\\\" [@rcoreteam2024]. In essence, the algorithm for  number generation \ncontained a bug that led to bias in the probabilities of numbers being chosen.^[We refer \ninterested readers to the thread reporting the bug and links contained within: [https://bugs.r-project.org/show_bug.cgi?id=17494](https://bugs.r-project.org/show_bug.cgi?id=17494)]. \nAs such, the correct version is the one created using R after version 3.6.0. \n\nA key takeaway, and focus of this paper, is that despite the authors providing \nreplication materials and indicating which version of \\texttt{R} they believed \nthey used, the results cannot be reproduced. This aspect is not a criticism of the \nauthors, who were unknowingly using software that contained an error, but rather a \ndemonstration of the challenges of reproducibility in the face of software updates.^[It \nis important to note that these kinds of breaking changes R are relatively \ninfrequent and breaking changes are more likely to come from updated packages.] \nThere are ways to ensure that the estimates are consistent (such as ensuring to always \nexplicitly articulate options in functions so that changes to defaults cannot break\nexisting code. However, reproducibility is still threatened by software updates \nthat result in larger changes (such as what inputs are required). Manually documenting \nsoftware versions (including packages or commands) in the code or README files \nis a good start, but as our example shows, it is not enough to ensure faithful\nreproduction. It should also be noted that the package that provides the \n\\texttt{Zelig()} function, \\texttt{Zelig} is not available anymore. It is still\npossible to install it from the CRAN archives, but this constitutes an additional\nrisk that must be managed for reproducibility.\n\n### Result sensitivity to seed\n\nGiven that the results obtained by @davenport2022multiracial changed when the way \nthat numbers are generated in `R` changed in version 3.6.0, we sought to check \nwhether the results are robust to different seeds or if this change was simply \nbad luck.\n\nFigure \\ref{fig-m2m_sim} plots the distribution of the means obtained from running the\ncode used to create Figure \\ref{dfifig1} 100 times with 100 different random seeds \non R version 3.5. Each panel in this figure shows the results for the models comparing \nmultiracial attitudes to those of monoracial minorities along a different measure of attitudes. \nThe solid black line plots the distribution of point estimates, the solid red vertical\nlines are the point estimates from Figure \\ref{dfifig1}, while the dashed red \nvertical lines are the 95% confidence intervals. These results suggest the estimates\nobtained by @davenport2022multiracial are very similar to those that would have been \ngenerated using any other seed. The estimate for the social welfare comparison between \nwhite-Asians to Asians is perhaps somewhat more negative (and thus less liberal) than \nwas found using the original seed. This makes sense, given that our results in Figure\n\\ref{postfig2} shift towards being more liberal for this comparison. To us, this \nsuggests that the authors' original conclusions for these comparisons are reasonably \nnot dramatically impacted by the specific seed used.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Solid black line splots distribution of estimates of multiracials’ attitudes, relative to monoracial minority background. 100 repetitions with 100 random seeds. Solid red line represents points estimates and dashed red line reflects 95% confidence intervals from @davenport2022multiracial.](rodrigues_wright_versioning_files/figure-pdf/fig-m2m_sim-1.pdf){#fig-m2m_sim}\n:::\n:::\n\n\nFigure \\ref{fig-m2w_sim} similarly plots the distribution of estimates obtained \nfrom 100 runs with 100 different random seeds on R version 3.5 for the models \ncomparing multiracial attitudes to those of monoracial whites. As above, the solid \nblack line plots the distribution of point estimates, the solid red vertical lines \nare the point estimates from Figure \\ref{dfifig1} while the dashed red vertical \nlines are the 95% confidence intervals. Each panel corresponds to a different \ncomparison between racial groups and a different measure of attitudes. Similarly\nto Figure \\ref{fig-m2m_sim}, the original estimates for White-Asian social welfare \nattitudes are less liberal than those suggested by other seeds, however the other \nestimates are in line with those from our many seeds exercise.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Solid black line splots distribution of estimates of multiracials’ attitudes, relative to monoracial white background. 100 repetitions with 100 random seeds. Solid red line represents points estimates and dashed red line reflects 95% confidence intervals from @davenport2022multiracial.](rodrigues_wright_versioning_files/figure-pdf/fig-m2w_sim-1.pdf){#fig-m2w_sim}\n:::\n:::\n\n\nIn summary, the results obtained by @davenport2022multiracial are not able to be \nrecreated despite providing code and a version number for `R`. This is due to a \nbug fix in `R` that changed default behavior when generating numbers. As the authors\nuse code that generates numbers in this way, they could be affected by the possible \nbias arising from the bug which was present in the version of `R` they used. In order \nto examine this, we ran their code 100 times using 100 random seeds and plotted the \nresults against the original estimates. We conclude that the original results appear\nreasonable and are not driven by the bug that was fixed in `R`.\n\n\\newpage\n\n## Discussion {#sec-discussion}\n### Building a development environment using Docker \nReplicating results from past studies is quite challenging, for many reasons.\nThis article focuses on one of these reasons: results cannot be reproduced\nbecause of changes introduced in more recent versions of the software used for\nanalysis, despite the availability of both data and the author' original\nscripts.\n\nTo reproduce the results from the original study and to pinpoint the impact of\nthe change introduced in \\texttt{R} 3.6.0, we chose to use Docker. To install Docker\n\nDocker is a containerization tool that enables one to build so-called *images*. The\nimages are usually built on top of a Linux operating system and contain all of\nthe required software to build any type of output. The images are made up of the \nsoftware, dependencies, code, etc., that will be used to generate the analysis. Think of \nthe image like a big sandbox that gets frozen in time and doesn't change but has\nall of the shovels and buckets that we need ready to go. After the image containing \nthe output has been built, users in turn only need to be able to run Docker \n*containers* instantiated from the image definition to build the output. \nContainers are basically one instance of an image, every time we step foot into\nthat sandbox do create something, we start from the same position where it was \nfrozen in time. If we want to change that start position, we need to rebuild the \nsandbox slightly differently. Containerization tools such as Docker solved the \n\\`\\`works on my machine\\\" problem: this problem arises when software that works \nwell on the development machine of the developer fails to run successfully on a \ncustomer's machine. This usually happens because the customer's computer does not \nhave the necessary dependencies to run the software (which are traditionally not \nshipped alongside the software product) or because of a version mismatch of the \noperating system.\n\nThe same approach can be used to build research projects, which also suffer\nfrom the same \\`\\`works on my machine\\\" problem as any other type of software.\nContainerization tools offer a great opportunity for reproducibility in\nresearch: instead of just sharing a replication script, authors can now easily\nshare the right version of the software used to produce these scripts, as well\nas the right version of the used libraries by building and sharing a Docker\nimage (or at least provide the necessary blueprint to enable others to do so, as\nwe will discuss below). Future researchers looking to reproduce the results can\nnow simply run a container from the provided image (or build an image themselves\nif the original authors provided the required blueprints).\n\nConcretely, to build a Docker image, a researcher writes a so-called\n*Dockerfile*, which lists all the necessary steps to build an output. Here is an\nexample of a very simple Dockerfile:\n\n```\nFROM rocker/r-ver:4.3.0\n\nCMD [\"R\"]\n```\n\nThis Dockerfile contains two lines: the first line states which Docker image we\nare going to use as a base. Our image will be based on the `rocker/r-ver:4.3.0`\nimage. The Rocker project [@boettiger2017] is a repository containing many images that ship\ndifferent versions of R and packages pre-installed: so the image called\n`r-ver:4.3.0` is an image that ships \\texttt{R} version 4.3.0. The last line states which\ncommand should be run when the user runs a container defined from our image, so in\nthis case, simply the R interactive prompt. Concretely, the above image would\nallow someone to run R version 4.3.0 as a container, completely isolated from\nthe rest of their computer. This allows users to run different versions of R on\nthe same system. We can add more commands to run a complete research\nproject, from start to finish. Below is an example of a Dockerfile that runs an\nanalysis script, by first stating which base image is going to be used, and then\nadding packages, files and data to run the entire project:\n\n```\nFROM rocker/r-ver:4.3.0\n\nRUN R -e \"install.packages('dplyr')\"\n\nRUN mkdir /home/research_project\n\nRUN mkdir /home/research_project/data\n\nRUN mkdir /home/research_project/project_output\n\nRUN mkdir /home/research_project/shared_folder\n\nCOPY analyse_data.R /home/research_project/analyse_data.R\n\nCOPY dataset.csv /home/research_project/dataset.csv\n\nRUN cd /home/research_project && R -e \"source('analyse_data.R')\"\n\nCMD mv /home/research_project/project_output/* /home/research_project/shared_folder/\n```\n\nThis Dockerfile starts from the same base image, an image that ships R\nversion 4.3.0, then it installs the `{dplyr}` package, a popular R package for\ndata manipulation and it creates three directories (`mkdir` meaning \\`\\`make directory\"):\n\n- `/home/research_project`\n- `/home/research_project/project_output`\n- `/home/research_project/shared_folder`.\n\nThen, it copies the `analyse_data.R` script, which contains the actual analysis\nmade for the research project, and the `dataset.csv` file, into\nthe Docker image. The second-to-last line runs the script, and the last line\nmoves the outputs generated from running the `analyse_data.R` script to a folder\ncalled `shared_folder`. It is important to say that `RUN` statements will be\nexecuted as the image gets built, and `CMD` statements will be executed as a\ncontainer runs. Using this Dockerfile, an image called `research_project` can be\nbuilt with the following command:\n\n```\ndocker build -t research_project .\n```\n\nThis image can then be archived and shared for replication purposes. Other\nimages can be built on top of this one, and will thus contain R version 4.3.0,\nthe `{dplyr}` package, the data and the project’s folder structure. Future\nresearchers can also run a container from that image using a command such as:\n\n```\ndocker run -d -it --rm --name research_project_container \\\n  -v /host/machine/shared_folder:/home/research_project/shared_folder:rw \\\n  research_project\n```\n\nThe container, called `research_project_container` will execute the `CMD`\nstatement from the Dockerfile, in other words, move the outputs to the\n`shared_folder`. This folder is like a tunnel between the machine that runs the\ncontainer and the container itself: by doing this, the outputs generated within\nthe container can now be accessed from the host's computer.\n\nThe image built by the process above is immutable: so as long as users can run\nit, the outputs produced will be the same as when the original author\nran the original analysis. However, if the image gets lost, and needs to be\nrebuilt, the above `Dockerfile` will not generate the same image. This is because\nthe `Dockerfile`, as it is written above, will download the version of `{dplyr}`\nthat is current at the time it gets built. So if a user instead builds the image\nin 5 years, the version of `{dplyr}` that will get downloaded will not be the\nsame as the one that was used for the original analysis. The version of\n\\texttt{R}, however, will forever remain at version 4.3.0.\n\nSo to ensure that future researchers will download the right versions of\npackages that were originally used for the project, the original researcher also\nneeds to provide a list of packages that were used as well as the packages'\nversions. This can be quite tedious if done by hand, but thankfully, there are\nways to generate such lists very easily. The `{renv}` package for the \\texttt{R}\nprogramming language provides such a function. Once the project is done, one\nsimply needs to call:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrenv::init()\n```\n:::\n\n\n\nto generate a file called `renv.lock`. This file contains the \\texttt{R} version that was\nused to generate it, the list of packages that were used for the project, as\nwell as their versions and links to download them. This file can be used to\neasily install all the required packages in the future by simply running:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrenv::restore()\n```\n:::\n\n\n\nA researcher can thus add the following steps in the Dockerfile to download the\nright packages when building the image:\n\n```\nCOPY renv.lock /home/research_project/renv.lock\n\nRUN R -e \"setwd('/home/research_project);renv::init();renv::restore()\"\n```\n\n\\newpage\n\n## Conclusion {#sec-conclusion}\n\nResearchers have to deal with the following aspects of\ntheir projects to ensure their long-term reproducibility:\n\n- They need to state the correct versions of R (or any other language) used for the analysis;\n- They need to state the correct versions of every other used package for the analysis;\n- They need to make sure that their code is readable and easy to explore; and\n- They need to provide instructions on how to run the whole project.\n\nBy using Docker, it is possible to provide the correct versions of \\texttt{R} and\npackages (using \\texttt{renv}, \\texttt{groundhog} or a snapshot of the CRAN repository).\nAnother important way to improve reproducibility is also to set up research projects\nas the output of pipelines instead of scripts, but this is outside the scope of this paper.\nBut this is not only useful for making projects\nreproducible: by using these tools, researchers also avoid common pitfalls while\nworking on their projects. Research projects can last for years and researchers\ntypically work on several projects in parallel; by using the tools discussed in\nthis article, researchers can thus have project-specific \\texttt{R} and package versions,\nthus avoiding the common issue of updating their package library because they\nneed the latest feature of a package for one project, but because packages got\nupdated, another project now produces different results or simply does not run\nanymore.\n\nHowever, we realize that the entry cost to a tool like Docker is not low: so at\nthe very least, we advise researchers interested in reproducibility to be very\nconservative with updating R itself and to use either \\texttt{renv} or \\texttt{groundhog} to\nhave project-specific libraries. This is because if researchers at least provide\nthe R version used and an renv.lock file (if they used \\texttt{renv}, or a date, if\nthey used \\texttt{groundhog}) it is always possible to re-generate an environment with\nthe right version of \\texttt{R} and packages as we did for this paper. This solution is\nnot the gold standard of reproducibility though, but easy enough to be\nimplemented at a low cost. This also implies that researchers need to share\ntheir code and all other necessary information on a platform such as Github, and\nit implies that research has to be performed using free and open source tools as\nproprietary tools cannot easily be installed by other researchers wanting to\nreproduce a study.\n\nA final issue that we want to address is the over-reliance on Docker. While\nwe have several alternatives for handling packages, Docker was the only solution\nthat we suggested to essentially \\`\\`freeze\\\" versions of R and other lower-level\nsystem dependencies. We want to finish this conclusion by stating that other tools \nachieve the same purpose, notably Nix [@dolstra2006purely]. Nix\nis not a containerization tool like Docker, but a package manager that allows\nusers to install exact versions of software, including scientific software,\nreproducibly. It is possible to essentially use Nix to replace Docker and \\texttt{renv}\n(or \\texttt{groundhog}), but presenting this tool is outside the scope of this article.\n\n\\newpage\n\n## References\n\n::: {#refs}\n:::\n\n\\newpage\n\\appendix\n\\renewcommand{\\thefigure}{A\\arabic{figure}}\n\\renewcommand{\\thetable}{A\\arabic{table}}\n\\setcounter{figure}{0}\n\\setcounter{table}{0}\n\n## Appendix\n### Rebuilding the original development environment using Docker {.appendix}\n\nSection @sec-discussion outlined what steps a researcher starting a new project \nshould take to set up a reproducible environment using Docker and `renv`.\nHowever, what should researchers who want to replicate a past study do if the \noriginal researcher did not provide a Dockerfile nor an `renv.lock` file? This\nis exactly the challenge that we were facing when trying to replicate the\nresults of @davenport2022multiracial. We needed to find a way to first install\nthe right version of \\texttt{R}, then the right version of the packages that they used,\nrun their original script, and then repeat this procedure but this time on a\nrecent version of \\texttt{R}.\n\nIn order to achieve this, we used a Docker image provided by the [R Installation\nManager](https://github.com/r-lib/rig)^[https://github.com/r-lib/rig] project.\nThis Docker image includes a tool, called `rig`, that makes it easy to switch \\texttt{R}\nversions, so we used it to first define an image that would use \\texttt{R} version 3.5.0\nby default as a base. Here are the three commands from the Dockerfile to achieve\nthis:\n\n```\nFROM rhub/rig:latest\n\nRUN rig install 3.5.0\n\nRUN rig default 3.5.0\n```\n\nThen, we had to install the packages that the original authors used to perform\ntheir analysis. We had to make some assumptions: since we only had the list of\nused packages, but not their exact versions, we assumed that the required\npackages were installed one year before the paper was published, so sometime in\nMay 2019. With this assumption, we then used the [Posit Package\nManager](https://packagemanager.posit.co/client/#/repos/2/overview)^[https://packagemanager.posit.co/client/#/repos/2/overview],\nwhich provides snapshots of CRAN that can be used to install \\texttt{R} packages as they\nwere on a given date. We thus configured \\texttt{R} to download packages from the\nsnapshot taken on May 16th, 2019. Then, the original replication script gets\nexecuted at image build time and we can obtain the outputs from running a\ncontainer from this image definition. With this setup, it was very simple to\nonly switch \\texttt{R} versions and re-execute everything. We simply had to switch the\ncommands from the Dockerfile:\n\n```\nFROM rhub/rig:latest\n\nRUN rig install 4.2.0\n\nRUN rig default 4.2.0\n```\n\nEverything else: package versions and operating system that the replication\nscript runs on, stayed the same.\n\nWith this setup, we were able to run the original analysis on an\nenvironment that was as close as possible to the original environment used by\n@davenport2022multiracial. However, it is impossible to regenerate the exact\nenvironment now. As stated, we had to make an assumption on the date the\npackages were downloaded, but the packages used by the original authors might\nhave been much older. Another likely difference is that the operating system\nused inside Docker is the Ubuntu Linux distribution. While Ubuntu is a popular\nLinux distribution, it is much more likely that the original authors used either\nWindows or macOS to perform their analysis. In most cases, the operating system\ndoes not have an impact on results, but there have been replication studies that\nfailed because of this, as in @neupane2019. Thankfully, the mismatch of the\noperating system does not seem to be an issue here.\n\nIt should also be noted that strictly speaking, using Docker is not completely\nreproducible either: this is because the underlying base image, upon which\nwe build our complete analysis, also changes through time. In the examples above\nthe image we used, `rhub/rig:latest` is based on Ubuntu 22.04, which is, as of\nwriting, the latest long-term support release of the Ubuntu operating system. This\nversion of Ubuntu thus still gets updated: so when building the image for our project,\nthe underlying base image will be different today than 6 months from now. And\nwhen the next Ubuntu long-term support will be released, in April 2024, then \n`rhub/rig:latest` will this time be based upon this new release of Ubuntu. So building\nin May 2024 will yield a different image than now. To avoid this, it is\npossible to use a so-called *digest* which ensures the same image will\nforever be used as a base. For this paper, we used the following\nimage: `rhub/rig@sha256:0667321839be0c2ade5c14fd8c8bbb311a843ddf690081c5d0dd80dcc98bd6c6`.\n\nFinally, to ensure completeness, instead of using the Posit Package Manager with a fixed date,\nwe could have used the `{groundhog}` package by @simonsohn2023. This package makes it possible to\ndownload packages as of a certain date as well, and it does not require users\nto change any of \\texttt{R}'s default configurations.\n",
    "supporting": [
      "rodrigues_wright_versioning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}